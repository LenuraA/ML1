 <h1> Алгоритм 1NN </h1>

<p>Алгоритм ближайшего соседа - 1NN  относит классифицируемый объект $u \in X^{l}$ к тому классу , к которому относится его ближайший сосед.</p>
<p> \begin{align*}
w(i,u) = [i=1]; ~  a(u; X^{l})=y_{u}^{(1)} 
\end{align*} </p>

<p> Приемущество: </p>
<p> Простота </p>

<p> Недостатки: </p>
 <p> Неустойчивость к погрешностям — выбросам.</p> 
 <p>
     Отсутствие параметров, которые можно было бы настраивать по выборке. Алгоритм полностью зависит от того, насколько удачно выбрана
метрика *ρ*.
</p>

<p>
  1. Нужно задать метрическую функцию. </p>
  <p>2. Обучающая выборка сортируется в порядке увеличения расстояния от классифицируемого элемента. </p>
 <p> 3. Классифицируемый элемент относим к классу, к которому принадлежит ближайший элемент(первый в отсортированной выборке). 
  </p>
  
 
  <p>Метрические алгоритмы классификации с обучающей выборкой *Xl* относят объект u к тому классу y, для которого суммарный вес ближайших обучающих объектов  $$ W_{y} (u, X^{l})$$ максимален: 
  $$ W_{y} (u, X^{l}) = \sum_{i:yu^{(i)}=y}^{} w(i,u) \to \max$$ </p>
  
  <p> , где весовая функция *w(i, u)* оценивает степень важности *i*-го соседа для классификации объекта *u*.</p>
  <p> Функция $$ W_{y} (u, X^{l})$$  называется оценкой близости объекта $u$ к классу $y$. Выбирая различную весовую функцию *w(i, u)* можно получать различные метрические классификаторы.</p>
  
  ```R
  distances = function(xl, data, k) { # возвращает отсортированный набор данных по метрике для объекта 
 cases = dim(data)[1]
 features = dim(data)[2]-1
 dists = matrix(0, cases, 2) # создаем матрицу расстояний 
   for (i in 1:cases) {
     cost = k(xl, data[i,1:features])
     dists[i,] = c(cost, i)
   }
   idx = order(dists[,1])
   data[dists[idx,2],]
 }
NN = function(xl, data, k=dist) 
 { # находит 1-ближайшего соседа 
 sorted = distances(xl, data, k)
   sorted[1,features+1]
 }
 ```
 
 ![screenshot of sample](https://github.com/LenuraA/ML1/blob/master/1.1nn.png)


<h1>Алгоритм k ближайших соседей </h1>
<p> Алгоритм k ближайших соседей – kNN относит объект $u$ к тому классу,
элементов которого больше среди $k$ ближайших соседей $x_{u}^{(i)},$ $i= 1, \dots,k$.
</p>

<p> Алгоритм работает следующим образом: пусть дан классифицируемый объект $u$ и обучающая выборка . Требуется определить класс объекта $u$ на основе данных из обучающей выборки. Для этого:</p>

<p>1. Вся выборка  сортируется по возрастанию расстояния от объекта $u$ до каждого объекта выборки.</p>
<p>2. Проверяются классы $k$ ближайших соседей объекта $u$. Класс, встречаемый наиболее часто, присваивается объекту $u$. Для оценки близости классифицируемого объекта $u$ к классу  алгоритм kNN использует следующую функцию: $W(i,u)=[i\leq k]$ , где $i$ - порядок соседа по расстоянию к классифицируемому объекту $u$.</p>

```R
kNN = function(xl, data, k, m=dist) { # находит k-ближайших соседей  
 sorted = distances(xl, data, m)
   
   n = 10 
   counts = rep(0, times=n)
 features = dim(data)[2]-1
 for (i in 1:k) {
     cls = sorted[i,features+1]
     counts[cls] = counts[cls] + 1
   }
   argmax = 1
   for (i in n) {
     if (counts[argmax] < counts[i]) {
       argmax = i
     }
   }
   
   cls[argmax]
 }
 ```
 
 ![screenshot of sample](https://github.com/LenuraA/ML1/blob/master/KNN.png)

**Алгоритм kNN выглядит более качествеено. Для того чтобы привести более точное обоснование чем kNN лучше в этом случае, чем 1NN, следует прибегнуть к скользящему контролю.
