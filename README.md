 ## Алгоритм 1NN

Алгоритм ближайшего соседа - 1NN  относит классифицируемый объект ![screenshot of sample](https://github.com/LenuraA/ML1/blob/master/формула1.png) к тому классу , к которому относится его ближайший сосед.
 ![screenshot of sample]( https://github.com/LenuraA/ML1/blob/master/формула2.png) 

## Приемущество: 
 Простота 

## Недостатки: 
  Неустойчивость к погрешностям — выбросам.
 Отсутствие параметров, которые можно было бы настраивать по выборке. Алгоритм полностью зависит от того, насколько удачно выбрана
метрика *ρ*.



 1. Нужно задать метрическую функцию. 
 2. Обучающая выборка сортируется в порядке увеличения расстояния от классифицируемого элемента.  
 3. Классифицируемый элемент относим к классу, к которому принадлежит ближайший элемент(первый в отсортированной выборке). 
  
  
 
  Метрические алгоритмы классификации с обучающей выборкой *Xl* относят объект *u* к тому классу *y*, для которого суммарный вес ближайших обучающих объектов  ![screenshot of sample](https://github.com/LenuraA/ML1/blob/master/формула3.png) максимален: 
  ![screenshot of sample](https://github.com/LenuraA/ML1/blob/master/формула4.png)
  
  
   , где весовая функция *w(i, u)* оценивает степень важности *i*-го соседа для классификации объекта *u*.
   Функция ![screenshot of sample]( https://github.com/LenuraA/ML1/blob/master/формула5.png) называется оценкой близости объекта *u* к классу *y*. Выбирая различную весовую функцию *w(i, u)* можно получать различные метрические классификаторы.
  
  ```R
  distances = function(xl, data, k) { # возвращает отсортированный набор данных по метрике для объекта 
 cases = dim(data)[1]
 features = dim(data)[2]-1
 dists = matrix(0, cases, 2) # создаем матрицу расстояний 
   for (i in 1:cases) {
     cost = k(xl, data[i,1:features])
     dists[i,] = c(cost, i)
   }
   idx = order(dists[,1])
   data[dists[idx,2],]
 }
NN = function(xl, data, k=dist) 
 { # находит 1-ближайшего соседа 
 sorted = distances(xl, data, k)
   sorted[1,features+1]
 }
 ```
 
 ![screenshot of sample](https://github.com/LenuraA/ML1/blob/master/1.1nn.png)


## Алгоритм k ближайших соседей 
 Алгоритм k ближайших соседей – kNN относит объект *u* к тому классу,
элементов которого больше среди *k* ближайших соседей  ![screenshot of sample](https://github.com/LenuraA/ML1/blob/master/формула6.png)


 Алгоритм работает следующим образом: пусть дан классифицируемый объект *u* и обучающая выборка . Требуется определить класс объекта *u* на основе данных из обучающей выборки. Для этого:

1. Вся выборка  сортируется по возрастанию расстояния от объекта *u* до каждого объекта выборки.
2. Проверяются классы *k* ближайших соседей объекта *u*. Класс, встречаемый наиболее часто, присваивается объекту *u*. Для оценки близости классифицируемого объекта *u* к классу  алгоритм kNN использует следующую функцию: ![screenshot of sample](https://github.com/LenuraA/ML1/blob/master/формула7.png) , где *i* - порядок соседа по расстоянию к классифицируемому объекту *u*.

```R
kNN = function(xl, data, k, m=dist) { # находит k-ближайших соседей  
 sorted = distances(xl, data, m)
   
   n = 10 
   counts = rep(0, times=n)
 features = dim(data)[2]-1
 for (i in 1:k) {
     cls = sorted[i,features+1]
     counts[cls] = counts[cls] + 1
   }
   argmax = 1
   for (i in n) {
     if (counts[argmax] < counts[i]) {
       argmax = i
     }
   }
   
   cls[argmax]
 }
 ```
 
 ![screenshot of sample](https://github.com/LenuraA/ML1/blob/master/KNN.png)

Алгоритм kNN выглядит  качествеено. Для того чтобы привести более точное обоснование чем kNN лучше в этом случае, чем 1NN, следует прибегнуть к скользящему контролю.
